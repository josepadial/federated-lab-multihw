{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354e308e",
   "metadata": {},
   "source": [
    "# 02 Â· Inference Engines Benchmark\n",
    "\n",
    "Load ONNX models from models_saved/onnx and benchmark on: PyTorch (CPU/CUDA), ONNX Runtime (CPU/CUDA), OpenVINO (CPU/GPU/NPU).\n",
    "Measure latency, throughput, accuracy, and energy when possible (NVML for NVIDIA). Save to a single CSV and avoid duplicates via model hash + configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46641f96af7a8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime diagnostics\n",
    "import os\n",
    "\n",
    "os.environ.setdefault(\"LOG_LEVEL\", \"INFO\")\n",
    "from utils.logging_utils import get_logger\n",
    "from utils.io import runtime_versions\n",
    "\n",
    "logger = get_logger(\"nb02\")\n",
    "versions = runtime_versions()\n",
    "logger.info(\"Runtime versions: %s\", versions)\n",
    "try:\n",
    "    import openvino as ov\n",
    "\n",
    "    logger.info(\"OpenVINO version: %s, convert_model=%s\", getattr(ov, '__version__', '?'), hasattr(ov, 'convert_model'))\n",
    "except Exception as ex:\n",
    "    logger.warning(\"OpenVINO import failed: %s\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51479a9d9ea9fbf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:57:20.990418Z",
     "start_time": "2025-08-13T09:52:15.079346Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:52:16 | INFO | nb02 | Benchmark: model=cnn engine=pytorch provider=cpu\n",
      "2025-08-13 11:52:23 | INFO | nb02 | Benchmark: model=cnn engine=pytorch provider=cuda\n",
      "2025-08-13 11:52:34 | INFO | nb02 | Benchmark: model=cnn engine=onnxruntime provider=cpu\n",
      "2025-08-13 11:52:41 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\cnn_cifar10.onnx | provider=CPUExecutionProvider\n",
      "2025-08-13 11:52:54 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 116.85089200094808, 'lat_ms_p95': 168.18604500294896, 'thr_ips': 547.7065592231913, 'acc': 0.828125}\n",
      "2025-08-13 11:52:54 | INFO | nb02 | Benchmark: model=cnn engine=onnxruntime provider=cuda\n",
      "2025-08-13 11:53:00 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\cnn_cifar10.onnx | provider=CUDAExecutionProvider\n",
      "2025-08-13 11:53:02 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 8.631570999568794, 'lat_ms_p95': 21.060824987944212, 'thr_ips': 7414.640973606918, 'acc': 0.828125}\n",
      "2025-08-13 11:53:02 | INFO | nb02 | Benchmark: model=cnn engine=openvino provider=CPU\n",
      "2025-08-13 11:53:09 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\cnn_cifar10.xml\n",
      "2025-08-13 11:53:09 | INFO | infer_openvino | Compiling model for device: CPU\n",
      "2025-08-13 11:53:13 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 23.317490000627004, 'lat_ms_p95': 92.67840500979217, 'thr_ips': 2744.720808212164, 'acc': 0.828125}\n",
      "2025-08-13 11:53:13 | INFO | nb02 | Benchmark: model=cnn engine=openvino provider=GPU\n",
      "2025-08-13 11:53:19 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\cnn_cifar10.xml\n",
      "2025-08-13 11:53:19 | INFO | infer_openvino | Compiling model for device: GPU\n",
      "2025-08-13 11:53:22 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 5.295112000894733, 'lat_ms_p95': 10.380605013051534, 'thr_ips': 12086.618751253176, 'acc': 0.828125}\n",
      "2025-08-13 11:53:22 | INFO | nb02 | Benchmark: model=cnn engine=openvino provider=NPU\n",
      "2025-08-13 11:53:28 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\cnn_cifar10.xml\n",
      "2025-08-13 11:53:28 | INFO | infer_openvino | Compiling model for device: NPU\n",
      "2025-08-13 11:53:29 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 2.3913170024752617, 'lat_ms_p95': 4.289545006758997, 'thr_ips': 26763.494732715633, 'acc': 0.09375}\n",
      "2025-08-13 11:53:29 | INFO | nb02 | Benchmark: model=mlp engine=pytorch provider=cpu\n",
      "2025-08-13 11:53:38 | INFO | nb02 | Benchmark: model=mlp engine=pytorch provider=cuda\n",
      "2025-08-13 11:53:47 | INFO | nb02 | Benchmark: model=mlp engine=onnxruntime provider=cpu\n",
      "2025-08-13 11:53:53 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mlp_cifar10.onnx | provider=CPUExecutionProvider\n",
      "2025-08-13 11:53:55 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 14.22537799749989, 'lat_ms_p95': 35.64225501031614, 'thr_ips': 4499.0017144885705, 'acc': 0.546875}\n",
      "2025-08-13 11:53:55 | INFO | nb02 | Benchmark: model=mlp engine=onnxruntime provider=cuda\n",
      "2025-08-13 11:54:02 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mlp_cifar10.onnx | provider=CUDAExecutionProvider\n",
      "2025-08-13 11:54:03 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 6.707603001268581, 'lat_ms_p95': 21.26930499944137, 'thr_ips': 9541.411438317973, 'acc': 0.546875}\n",
      "2025-08-13 11:54:03 | INFO | nb02 | Benchmark: model=mlp engine=openvino provider=CPU\n",
      "2025-08-13 11:54:10 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mlp_cifar10.xml\n",
      "2025-08-13 11:54:10 | INFO | infer_openvino | Compiling model for device: CPU\n",
      "2025-08-13 11:54:11 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 7.132767000875901, 'lat_ms_p95': 17.669544985983507, 'thr_ips': 8972.67497902859, 'acc': 0.546875}\n",
      "2025-08-13 11:54:11 | INFO | nb02 | Benchmark: model=mlp engine=openvino provider=GPU\n",
      "2025-08-13 11:54:18 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mlp_cifar10.xml\n",
      "2025-08-13 11:54:18 | INFO | infer_openvino | Compiling model for device: GPU\n",
      "2025-08-13 11:54:22 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 11.572126999380998, 'lat_ms_p95': 19.02132000395795, 'thr_ips': 5530.530385937123, 'acc': 0.546875}\n",
      "2025-08-13 11:54:22 | INFO | nb02 | Benchmark: model=mlp engine=openvino provider=NPU\n",
      "2025-08-13 11:54:29 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mlp_cifar10.xml\n",
      "2025-08-13 11:54:29 | INFO | infer_openvino | Compiling model for device: NPU\n",
      "2025-08-13 11:54:30 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 4.559163000376429, 'lat_ms_p95': 9.496330012916587, 'thr_ips': 14037.66436837547, 'acc': 0.09375}\n",
      "2025-08-13 11:54:30 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=pytorch provider=cpu\n",
      "2025-08-13 11:55:05 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=pytorch provider=cuda\n",
      "2025-08-13 11:55:20 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=onnxruntime provider=cpu\n",
      "2025-08-13 11:55:28 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mobilenetv3_cifar10.onnx | provider=CPUExecutionProvider\n",
      "2025-08-13 11:55:29 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 7.2979110007872805, 'lat_ms_p95': 9.218919980048668, 'thr_ips': 8769.632843302123, 'acc': 0.78125}\n",
      "2025-08-13 11:55:29 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=onnxruntime provider=cuda\n",
      "2025-08-13 11:55:36 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mobilenetv3_cifar10.onnx | provider=CUDAExecutionProvider\n",
      "2025-08-13 11:55:37 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 4.66417800023919, 'lat_ms_p95': 8.821154998440756, 'thr_ips': 13721.603248571973, 'acc': 0.78125}\n",
      "2025-08-13 11:55:37 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=openvino provider=CPU\n",
      "2025-08-13 11:55:44 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mobilenetv3_cifar10.xml\n",
      "2025-08-13 11:55:44 | INFO | infer_openvino | Compiling model for device: CPU\n",
      "2025-08-13 11:55:45 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 9.381026000774, 'lat_ms_p95': 10.88501000049291, 'thr_ips': 6822.281485492052, 'acc': 0.78125}\n",
      "2025-08-13 11:55:45 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=openvino provider=GPU\n",
      "2025-08-13 11:55:51 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mobilenetv3_cifar10.xml\n",
      "2025-08-13 11:55:51 | INFO | infer_openvino | Compiling model for device: GPU\n",
      "2025-08-13 11:55:53 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 4.37214099947596, 'lat_ms_p95': 5.029505012498703, 'thr_ips': 14638.137243897434, 'acc': 0.78125}\n",
      "2025-08-13 11:55:54 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=openvino provider=NPU\n",
      "2025-08-13 11:56:00 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mobilenetv3_cifar10.xml\n",
      "2025-08-13 11:56:00 | INFO | infer_openvino | Compiling model for device: NPU\n",
      "2025-08-13 11:56:00 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 1.235680000972934, 'lat_ms_p95': 1.4975949976360423, 'thr_ips': 51793.34451444427, 'acc': 0.09375}\n",
      "2025-08-13 11:56:00 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=pytorch provider=cpu\n",
      "2025-08-13 11:56:17 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=pytorch provider=cuda\n",
      "2025-08-13 11:56:29 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=onnxruntime provider=cpu\n",
      "2025-08-13 11:56:36 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\efficientnetlite0_cifar10.onnx | provider=CPUExecutionProvider\n",
      "2025-08-13 11:56:46 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 26.766133997880388, 'lat_ms_p95': 32.23674999026116, 'thr_ips': 2391.0812075090175, 'acc': 0.859375}\n",
      "2025-08-13 11:56:46 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=onnxruntime provider=cuda\n",
      "2025-08-13 11:56:52 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\efficientnetlite0_cifar10.onnx | provider=CUDAExecutionProvider\n",
      "2025-08-13 11:56:53 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 5.943152999388985, 'lat_ms_p95': 7.024135014216881, 'thr_ips': 10768.694665370356, 'acc': 0.859375}\n",
      "2025-08-13 11:56:53 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=openvino provider=CPU\n",
      "2025-08-13 11:57:00 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\efficientnetlite0_cifar10.xml\n",
      "2025-08-13 11:57:00 | INFO | infer_openvino | Compiling model for device: CPU\n",
      "2025-08-13 11:57:04 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 34.88348399871029, 'lat_ms_p95': 36.21838498220313, 'thr_ips': 1834.6791278751343, 'acc': 0.859375}\n",
      "2025-08-13 11:57:04 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=openvino provider=GPU\n",
      "2025-08-13 11:57:10 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\efficientnetlite0_cifar10.xml\n",
      "2025-08-13 11:57:10 | INFO | infer_openvino | Compiling model for device: GPU\n",
      "2025-08-13 11:57:13 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 16.224011999438517, 'lat_ms_p95': 16.6855149960611, 'thr_ips': 3944.770257949447, 'acc': 0.859375}\n",
      "2025-08-13 11:57:14 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=openvino provider=NPU\n",
      "2025-08-13 11:57:20 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\efficientnetlite0_cifar10.xml\n",
      "2025-08-13 11:57:20 | INFO | infer_openvino | Compiling model for device: NPU\n",
      "2025-08-13 11:57:20 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 1.7320920006022789, 'lat_ms_p95': 2.419469997403212, 'thr_ips': 36949.538464322955, 'acc': 0.09375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "# Imports and config\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from models.cnn import CNN\n",
    "from models.efficientnet_lite0 import EfficientNetLite0\n",
    "from models.mlp import MLP\n",
    "from models.mobilenetv3 import MobileNetV3\n",
    "from utils.consistency import compare_logits_torch_ort_ov\n",
    "from utils.device_utils import get_cpu_name, get_gpu_name_and_driver\n",
    "from utils.energy import GpuEnergyMeterNVML\n",
    "from utils.infer_openvino import benchmark_numpy as ov_bench\n",
    "from utils.infer_ort import benchmark_numpy as ort_bench\n",
    "from utils.io import CSV_SCHEMA, csv_append_row, csv_has_row_with, sha256_file, utc_timestamp, runtime_versions\n",
    "from utils.ov_convert import onnx_to_ir\n",
    "from utils.ov_utils import get_available_devices as ov_available\n",
    "from utils.preprocess import preprocess_np\n",
    "from utils.logging_utils import get_logger\n",
    "\n",
    "# Logging\n",
    "os.environ.setdefault(\"LOG_LEVEL\", \"INFO\")\n",
    "logger = get_logger(\"nb02\")\n",
    "\n",
    "root = Path(__file__).resolve().parent.parent if '__file__' in globals() else Path(os.getcwd()).parent\n",
    "\n",
    "\n",
    "def _default_cfg():\n",
    "    return {\n",
    "        'defaults': {\n",
    "            'dataset': 'cifar10', 'precision': 'fp32', 'batch': 64,\n",
    "            'warmup': 10, 'runs': 100, 'image_size': 32, 'num_workers': 2,\n",
    "        },\n",
    "        'engines': [\n",
    "            {'engine': 'pytorch', 'providers': ['cpu', 'cuda']},\n",
    "            {'engine': 'onnxruntime', 'providers': ['cpu', 'cuda']},\n",
    "            {'engine': 'openvino', 'providers': ['CPU', 'GPU']},\n",
    "        ],\n",
    "        'models': [\n",
    "            {'name': 'cnn', 'file_onnx': 'cnn_cifar10.onnx', 'file_pt': 'cnn_cifar10.pt'},\n",
    "            {'name': 'mlp', 'file_onnx': 'mlp_cifar10.onnx', 'file_pt': 'mlp_cifar10.pt'},\n",
    "            {'name': 'mobilenetv3', 'file_onnx': 'mobilenetv3_cifar10.onnx', 'file_pt': 'mobilenetv3_cifar10.pt'},\n",
    "            {'name': 'efficientnetlite0', 'file_onnx': 'efficientnetlite0_cifar10.onnx', 'file_pt': 'efficientnetlite0_cifar10.pt'},\n",
    "        ],\n",
    "        'outputs': {\n",
    "            'infer_csv': 'metrics/infer_metrics.csv'\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def load_cfg():\n",
    "    cfg_path = root / 'config/bench_matrix.yaml'\n",
    "    if cfg_path.exists():\n",
    "        try:\n",
    "            return yaml.safe_load(open(cfg_path, 'r', encoding='utf-8'))\n",
    "        except Exception as ex:\n",
    "            logger.warning(\"Failed to read config/bench_matrix.yaml, using defaults: %s\", ex)\n",
    "    return _default_cfg()\n",
    "\n",
    "\n",
    "cfg = load_cfg()\n",
    "defs = cfg['defaults']\n",
    "engines = cfg['engines']\n",
    "models_cfg = {m['name']: m for m in cfg['models']}\n",
    "out_csv = root / cfg['outputs']['infer_csv']\n",
    "onnx_dir = root / 'models_saved/onnx'\n",
    "pt_dir = root / 'models_saved/pytorch'\n",
    "versions = runtime_versions()\n",
    "os_str = versions['os']\n",
    "\n",
    "# Dataset (download used only to match labels; we will preprocess with our function)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "try:\n",
    "    testset = datasets.CIFAR10(root=f'{root}/data', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=defs['batch'], shuffle=False,\n",
    "                                              num_workers=defs['num_workers'])\n",
    "except Exception as ex:\n",
    "    logger.exception(\"Failed to prepare CIFAR10 test loader: %s\", ex)\n",
    "    raise\n",
    "\n",
    "\n",
    "def build_model(name: str):\n",
    "    if name == 'cnn': return CNN()\n",
    "    if name == 'mlp': return MLP(input_size=32 * 32 * 3)\n",
    "    if name == 'mobilenetv3': return MobileNetV3()\n",
    "    if name == 'efficientnetlite0': return EfficientNetLite0()\n",
    "    raise ValueError(f'Unknown model {name}')\n",
    "\n",
    "\n",
    "def bench_pytorch_single_batch(model, xb, yb, device, warmup: int, runs: int):\n",
    "    model = model.to(device).eval()\n",
    "    x_np = preprocess_np(xb.permute(0, 2, 3, 1).numpy())  # ensure NCHW normalized\n",
    "    x_t = torch.from_numpy(x_np).to(device)\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model(x_t)\n",
    "        if device.type == 'cuda': torch.cuda.synchronize()\n",
    "    # Timed\n",
    "    lat = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _ in range(runs):\n",
    "        t0 = time.perf_counter()\n",
    "        out = model(x_t)\n",
    "        if device.type == 'cuda': torch.cuda.synchronize()\n",
    "        dt = time.perf_counter() - t0\n",
    "        lat.append(dt)\n",
    "        pred = out.argmax(1).detach().cpu().numpy()\n",
    "        correct += int((pred == yb.numpy()).sum())\n",
    "        total += yb.shape[0]\n",
    "    lat_ms = np.array(lat) * 1000.0\n",
    "    lat_ms_mean = float(lat_ms.mean()) if lat_ms.size else 0.0\n",
    "    lat_ms_p95 = float(np.percentile(lat_ms, 95)) if lat_ms.size else 0.0\n",
    "    thr = float(total / sum(lat)) if lat else 0.0\n",
    "    acc = float(correct / total) if total else 0.0\n",
    "    return {\"lat_ms_mean\": lat_ms_mean, \"lat_ms_p95\": lat_ms_p95, \"thr_ips\": thr, \"acc\": acc}\n",
    "\n",
    "\n",
    "def make_ort_session(onnx_path: str, provider: str):\n",
    "    import onnxruntime as ort\n",
    "    providers = [provider]\n",
    "    return ort.InferenceSession(onnx_path, providers=providers)\n",
    "\n",
    "\n",
    "def make_ov_compiled(ir_path: str, device: str):\n",
    "    from openvino import Core\n",
    "    core = Core()\n",
    "    model = core.read_model(ir_path)\n",
    "    return core.compile_model(model, device)\n",
    "\n",
    "\n",
    "def consistency_check(model_name: str, onnx_path: Path, ir_path: Path, xb, device):\n",
    "    # Build single-batch normalized sample\n",
    "    x_np = preprocess_np(xb.permute(0, 2, 3, 1).numpy())\n",
    "    # Torch\n",
    "    model = build_model(model_name)\n",
    "    state = torch.load(pt_dir / models_cfg[model_name]['file_pt'], map_location=str(device))\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    model = model.to(device).eval()\n",
    "    # ORT sess\n",
    "    sess = make_ort_session(str(onnx_path),\n",
    "                            'CUDAExecutionProvider' if device.type == 'cuda' else 'CPUExecutionProvider')\n",
    "    # OV compiled\n",
    "    ov_comp = make_ov_compiled(str(ir_path), 'GPU' if device.type == 'cuda' else 'CPU')\n",
    "    res = compare_logits_torch_ort_ov(model, sess, ov_comp, x_np)\n",
    "    return res\n",
    "\n",
    "\n",
    "# --------------- helpers to reduce bench_once complexity ---------------\n",
    "\n",
    "def _normalize_provider(engine: str, provider: str) -> str:\n",
    "    return provider.upper() if engine in ('pytorch', 'onnxruntime') else provider\n",
    "\n",
    "\n",
    "def _cache_match(model_hash: str, engine: str, provider_csv: str, defs: dict, versions: dict, driver_ver: str) -> dict:\n",
    "    return {\n",
    "        \"model_hash\": model_hash,\n",
    "        \"engine\": engine,\n",
    "        \"provider\": provider_csv,\n",
    "        \"dataset\": defs['dataset'],\n",
    "        \"precision\": defs['precision'],\n",
    "        \"batch\": defs['batch'],\n",
    "        \"warmup\": defs['warmup'],\n",
    "        \"runs\": defs['runs'],\n",
    "        \"torch_ver\": versions['torch_ver'],\n",
    "        \"ort_ver\": versions['ort_ver'],\n",
    "        \"ov_ver\": versions['ov_ver'],\n",
    "        \"driver_ver\": driver_ver,\n",
    "        \"cached\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def _fixed_batch(loader):\n",
    "    xb, yb = next(iter(loader))\n",
    "    x_np = preprocess_np(xb.permute(0, 2, 3, 1).numpy())\n",
    "    return xb, yb, x_np, yb.numpy()\n",
    "\n",
    "\n",
    "def _ensure_ir(model_name: str, onnx_path: Path) -> Path | None:\n",
    "    ir_path = (root / 'models_saved/openvino_ir' / (models_cfg[model_name]['file_onnx'].replace('.onnx', '.xml')))\n",
    "    if ir_path.exists():\n",
    "        return ir_path\n",
    "    try:\n",
    "        logger.info(\"Converting to OpenVINO IR: %s -> %s\", onnx_path, ir_path)\n",
    "        return onnx_to_ir(onnx_path, ir_path.parent, ir_path.name,\n",
    "                          input_shape=(1, 3, defs['image_size'], defs['image_size']))\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"IR conversion failed: %s\", ex)\n",
    "        return None\n",
    "\n",
    "\n",
    "def _row_common(model_name, engine, provider_csv, cached, device_name, cpu_name, gpu_name, os_str, versions,\n",
    "                driver_ver, model_hash):\n",
    "    return {\n",
    "        'ts': utc_timestamp(), 'exp_id': 'engines-bench', 'model': model_name, 'dataset': defs['dataset'],\n",
    "        'precision': defs['precision'], 'engine': engine, 'provider': provider_csv, 'batch': defs['batch'],\n",
    "        'warmup': defs['warmup'], 'runs': defs['runs'], 'cached': cached, 'device_name': device_name,\n",
    "        'cpu_name': cpu_name, 'gpu_name': gpu_name, 'os': os_str, 'torch_ver': versions['torch_ver'],\n",
    "        'ort_ver': versions['ort_ver'], 'ov_ver': versions['ov_ver'], 'driver_ver': driver_ver,\n",
    "        'model_hash': model_hash,\n",
    "    }\n",
    "\n",
    "\n",
    "def _write_cached_row(out_csv, base):\n",
    "    row = {**base,\n",
    "           'lat_ms_mean': '', 'lat_ms_p95': '', 'thr_ips': '', 'acc': '', 'energy_j': 'N/D',\n",
    "           'consistency_ok': '', 'max_abs_diff_torch_ort': '', 'max_abs_diff_torch_ov': '',\n",
    "           'top1_agree_torch_ort': '', 'top1_agree_torch_ov': ''}\n",
    "    csv_append_row(str(out_csv), row, CSV_SCHEMA)\n",
    "\n",
    "\n",
    "def _ov_provider_available(provider: str) -> bool:\n",
    "    try:\n",
    "        return provider in set(ov_available())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _run_pytorch(model_name, onnx_path, xb, yb, x_np, cpu_name, device_pref):\n",
    "    dev = 'cuda' if device_pref.lower() == 'cuda' and torch.cuda.is_available() else 'cpu'\n",
    "    model = build_model(model_name)\n",
    "    state = torch.load(pt_dir / models_cfg[model_name]['file_pt'], map_location=dev)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    device = torch.device(dev)\n",
    "    # Energy\n",
    "    meter = GpuEnergyMeterNVML(0) if (device.type == 'cuda') else None\n",
    "    if meter:\n",
    "        def _one():\n",
    "            _ = model.to(device)(torch.from_numpy(x_np).to(device))\n",
    "            torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        e_j, _ = meter.measure(_one)\n",
    "    else:\n",
    "        e_j = -1.0\n",
    "    metrics = bench_pytorch_single_batch(model, xb, yb, device=device, warmup=defs['warmup'], runs=defs['runs'])\n",
    "    energy_val = e_j if e_j >= 0 else 'N/D'\n",
    "    # Consistency\n",
    "    ir_path = _ensure_ir(model_name, onnx_path)\n",
    "    consistency = {'consistency_ok': False, 'max_abs_diff_torch_ort': '', 'max_abs_diff_torch_ov': '',\n",
    "                   'top1_agree_torch_ort': '', 'top1_agree_torch_ov': ''}\n",
    "    if ir_path is not None and ir_path.exists():\n",
    "        try:\n",
    "            consistency = consistency_check(model_name, onnx_path, ir_path, xb, device)\n",
    "        except Exception as ex:\n",
    "            logger.warning(\"Consistency check failed: %s\", ex)\n",
    "    device_name = (torch.cuda.get_device_name(0) if device.type == 'cuda' else cpu_name)\n",
    "    return {\"status\": \"ok\", \"metrics\": metrics, \"energy_j\": energy_val, \"device_name\": device_name,\n",
    "            \"consistency\": consistency}\n",
    "\n",
    "\n",
    "def _run_ort(onnx_path, x_np, provider, y, cpu_name):\n",
    "    prov = 'CUDAExecutionProvider' if provider.lower() == 'cuda' else 'CPUExecutionProvider'\n",
    "    # Energy\n",
    "    meter = GpuEnergyMeterNVML(0) if (prov == 'CUDAExecutionProvider' and torch.cuda.is_available()) else None\n",
    "    if meter:\n",
    "        def _one_inf():\n",
    "            sess = make_ort_session(str(onnx_path), prov)\n",
    "            _ = sess.run(None, {sess.get_inputs()[0].name: x_np})\n",
    "        e_j, _ = meter.measure(_one_inf)\n",
    "    else:\n",
    "        e_j = -1.0\n",
    "    # Metrics\n",
    "    try:\n",
    "        metrics = ort_bench(str(onnx_path), x_np, provider=prov, warmup=defs['warmup'], runs=defs['runs'], y_true=y)\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"ORT bench failed: %s\", ex)\n",
    "        metrics = {\"lat_ms_mean\": '', \"lat_ms_p95\": '', \"thr_ips\": '', \"acc\": ''}\n",
    "    device_name = (torch.cuda.get_device_name(0) if provider.lower() == 'cuda' else cpu_name)\n",
    "    return {\"status\": \"ok\", \"metrics\": metrics, \"energy_j\": (e_j if e_j >= 0 else 'N/D'),\n",
    "            \"device_name\": device_name, \"consistency\": None}\n",
    "\n",
    "\n",
    "def _run_ov(model_name, onnx_path, x_np, provider, y):\n",
    "    if not _ov_provider_available(provider):\n",
    "        return {\"status\": \"unavailable\"}\n",
    "    ir_path = _ensure_ir(model_name, onnx_path)\n",
    "    if ir_path is None:\n",
    "        return {\"status\": \"missing_ir\"}\n",
    "    metrics = ov_bench(str(ir_path), x_np, device=provider, warmup=defs['warmup'], runs=defs['runs'], y_true=y)\n",
    "    return {\"status\": \"ok\", \"metrics\": metrics, \"energy_j\": 'N/D', \"device_name\": provider, \"consistency\": None}\n",
    "\n",
    "\n",
    "def _exec_engine(model_name, engine, provider, onnx_path, xb, yb, x_np, y, cpu_name):\n",
    "    if engine == 'pytorch':\n",
    "        return _run_pytorch(model_name, onnx_path, xb, yb, x_np, cpu_name, provider)\n",
    "    if engine == 'onnxruntime':\n",
    "        return _run_ort(onnx_path, x_np, provider, y, cpu_name)\n",
    "    if engine == 'openvino':\n",
    "        return _run_ov(model_name, onnx_path, x_np, provider, y)\n",
    "    return {\"status\": \"error\", \"error\": f\"Unknown engine {engine}\"}\n",
    "\n",
    "\n",
    "# --------------------------- minimized bench_once ---------------------------\n",
    "\n",
    "def bench_once(model_name: str, engine: str, provider: str):\n",
    "    onnx_path = onnx_dir / models_cfg[model_name]['file_onnx']\n",
    "    model_hash = sha256_file(str(onnx_path))\n",
    "    gpu_name, driver_ver = get_gpu_name_and_driver()\n",
    "    cpu_name = get_cpu_name()\n",
    "    provider_csv = _normalize_provider(engine, provider)\n",
    "\n",
    "    # Cache\n",
    "    match = _cache_match(model_hash, engine, provider_csv, defs, versions, driver_ver)\n",
    "    if csv_has_row_with(str(out_csv), match):\n",
    "        base = _row_common(model_name, engine, provider_csv, True, provider_csv, cpu_name, gpu_name, os_str, versions,\n",
    "                           driver_ver, model_hash)\n",
    "        _write_cached_row(out_csv, base)\n",
    "        return 'cached'\n",
    "\n",
    "    # Fixed batch\n",
    "    xb, yb, x_np, y = _fixed_batch(test_loader)\n",
    "\n",
    "    # Execute\n",
    "    res = _exec_engine(model_name, engine, provider, onnx_path, xb, yb, x_np, y, cpu_name)\n",
    "    status = res.get('status')\n",
    "\n",
    "    # Handle non-ok statuses (write informative rows)\n",
    "    if status in ('unavailable', 'missing_ir', 'error'):\n",
    "        device_name = provider_csv\n",
    "        base = _row_common(model_name, engine, provider_csv, False, device_name, cpu_name, gpu_name, os_str, versions,\n",
    "                           driver_ver, model_hash)\n",
    "        row = {**base, 'lat_ms_mean': '', 'lat_ms_p95': '', 'thr_ips': '', 'acc': '', 'energy_j': 'N/D',\n",
    "               'consistency_ok': False, 'max_abs_diff_torch_ort': '', 'max_abs_diff_torch_ov': '',\n",
    "               'top1_agree_torch_ort': '', 'top1_agree_torch_ov': ''}\n",
    "        csv_append_row(str(out_csv), row, CSV_SCHEMA)\n",
    "        return status\n",
    "\n",
    "    # OK path: write results\n",
    "    metrics = res['metrics']\n",
    "    energy_j = res['energy_j']\n",
    "    device_name = res['device_name']\n",
    "    base = _row_common(model_name, engine, provider_csv, False, device_name, cpu_name, gpu_name, os_str, versions,\n",
    "                       driver_ver, model_hash)\n",
    "    if engine == 'pytorch':\n",
    "        cons = res.get('consistency', {}) or {}\n",
    "        row = {**base, 'lat_ms_mean': metrics['lat_ms_mean'], 'lat_ms_p95': metrics['lat_ms_p95'],\n",
    "               'thr_ips': metrics['thr_ips'], 'acc': metrics['acc'], 'energy_j': energy_j,\n",
    "               'consistency_ok': cons.get('consistency_ok', False),\n",
    "               'max_abs_diff_torch_ort': cons.get('max_abs_diff_torch_ort', ''),\n",
    "               'max_abs_diff_torch_ov': cons.get('max_abs_diff_torch_ov', ''),\n",
    "               'top1_agree_torch_ort': cons.get('top1_agree_torch_ort', ''),\n",
    "               'top1_agree_torch_ov': cons.get('top1_agree_torch_ov', '')}\n",
    "    else:\n",
    "        row = {**base, 'lat_ms_mean': metrics['lat_ms_mean'], 'lat_ms_p95': metrics['lat_ms_p95'],\n",
    "               'thr_ips': metrics['thr_ips'], 'acc': metrics['acc'], 'energy_j': energy_j,\n",
    "               'consistency_ok': '', 'max_abs_diff_torch_ort': '', 'max_abs_diff_torch_ov': '',\n",
    "               'top1_agree_torch_ort': '', 'top1_agree_torch_ov': ''}\n",
    "    csv_append_row(str(out_csv), row, CSV_SCHEMA)\n",
    "    return 'done'\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for m in models_cfg.keys():\n",
    "    for e in engines:\n",
    "        eng = e['engine']\n",
    "        for prov in e['providers']:\n",
    "            logger.info(\"Benchmark: model=%s engine=%s provider=%s\", m, eng, prov)\n",
    "            try:\n",
    "                _ = bench_once(m, eng, prov)\n",
    "            except Exception as ex:\n",
    "                logger.exception(\"Error in bench_once: %s\", ex)\n",
    "print('Benchmark completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c56de0b06652fcc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:57:21.091602Z",
     "start_time": "2025-08-13T09:57:21.073218Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:57:21 | INFO | nb02 | Wrote energy summary: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\metrics\\inference_energy_summary.csv (rows=8)\n"
     ]
    }
   ],
   "source": [
    "# Summarize energy rows into a dedicated CSV for plots\n",
    "try:\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(out_csv)\n",
    "    energy_csv = root / 'metrics' / 'inference_energy_summary.csv'\n",
    "    from utils.io import ensure_dir\n",
    "\n",
    "    ensure_dir(str(energy_csv.parent))\n",
    "    cols = ['model', 'engine', 'provider', 'batch', 'runs', 'energy_j', 'device_name', 'gpu_name', 'cpu_name', 'os']\n",
    "    df_energy = df[df['energy_j'].astype(str) != 'N/D'][cols]\n",
    "    df_energy.to_csv(energy_csv, index=False)\n",
    "    logger.info(\"Wrote energy summary: %s (rows=%d)\", energy_csv, len(df_energy))\n",
    "except Exception as ex:\n",
    "    logger.warning(\"Energy summary not generated: %s\", ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8558cfc7cc2fad",
   "metadata": {},
   "source": [
    "## Notes (Windows assumptions)\n",
    "\n",
    "- No CPU energy via RAPL/PCM on Windows; energy_j is measured only on NVIDIA GPUs via NVML. Others are marked as \"N/D\".\n",
    "- NPU is optional; if not available, rows are recorded with provider status unavailable/missing.\n",
    "- This notebook writes a single CSV and prints sanity tables only; all plots are produced in `04_results_and_plots.ipynb`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
