{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354e308e",
   "metadata": {},
   "source": [
    "# 02 Â· Inference Engines Benchmark\n",
    "\n",
    "This notebook performs fair, reproducible benchmarking across PyTorch, ONNX Runtime, and OpenVINO on Windows. It uses a unified preprocessing pipeline, fixed warmup/runs, and a single cached CSV for results. Energy is measured on NVIDIA GPUs via NVML when available; other devices are marked as N/D.\n",
    "\n",
    "- Prereqs and context: see [overview](../docs/overview.md) and [getting started](../docs/getting-started.md)\n",
    "- Methodology: see [reproducibility](../docs/reproducibility.md) and [benchmarks](../docs/benchmarks.md)\n",
    "- How to interpret results and figures: see [results](../docs/results.md) and [energy](../docs/energy.md)"
   ]
  },
  {
   "cell_type": "code",
   "id": "46641f96af7a8661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:40:34.742364Z",
     "start_time": "2025-08-13T18:40:33.063298Z"
    }
   },
   "source": [
    "# Runtime diagnostics\n",
    "import os\n",
    "\n",
    "os.environ.setdefault(\"LOG_LEVEL\", \"INFO\")\n",
    "from utils.logging_utils import get_logger\n",
    "from utils.io import runtime_versions\n",
    "\n",
    "logger = get_logger(\"nb02\")\n",
    "versions = runtime_versions()\n",
    "logger.info(\"Runtime versions: %s\", versions)\n",
    "try:\n",
    "    import openvino as ov\n",
    "\n",
    "    logger.info(\"OpenVINO version: %s, convert_model=%s\", getattr(ov, '__version__', '?'), hasattr(ov, 'convert_model'))\n",
    "except Exception as ex:\n",
    "    logger.warning(\"OpenVINO import failed: %s\", ex)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 20:40:34 | INFO | nb02 | Runtime versions: {'torch_ver': '2.8.0+cu129', 'ort_ver': '1.22.0', 'ov_ver': '2025.2.0-19140-c01cd93e24d-releases/2025/2', 'os': 'Windows-10-10.0.26100-SP0'}\n",
      "2025-08-13 20:40:34 | INFO | nb02 | OpenVINO version: 2025.2.0-19140-c01cd93e24d-releases/2025/2, convert_model=True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "d14045be",
   "metadata": {},
   "source": [
    "Tip: If a provider is missing or a session fails to compile, refer to [troubleshooting](../docs/troubleshooting.md). For the full documentation set, see [README](../docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "id": "51479a9d9ea9fbf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:44:45.685943Z",
     "start_time": "2025-08-13T18:40:34.750372Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "# Imports and config\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from models.cnn import CNN\n",
    "from models.efficientnet_lite0 import EfficientNetLite0\n",
    "from models.mlp import MLP\n",
    "from models.mobilenetv3 import MobileNetV3\n",
    "from utils.consistency import compare_logits_torch_ort_ov\n",
    "from utils.device_utils import get_cpu_name, get_gpu_name_and_driver\n",
    "from utils.energy import GpuEnergyMeterNVML\n",
    "from utils.infer_openvino import benchmark_numpy as ov_bench\n",
    "from utils.infer_ort import benchmark_numpy as ort_bench\n",
    "from utils.io import CSV_SCHEMA, csv_append_row, csv_has_row_with, sha256_file, utc_timestamp, runtime_versions\n",
    "from utils.ov_convert import onnx_to_ir\n",
    "from utils.ov_utils import get_available_devices as ov_available\n",
    "from utils.preprocess import preprocess_np\n",
    "from utils.logging_utils import get_logger\n",
    "\n",
    "# Logging\n",
    "os.environ.setdefault(\"LOG_LEVEL\", \"INFO\")\n",
    "logger = get_logger(\"nb02\")\n",
    "\n",
    "root = Path(__file__).resolve().parent.parent if '__file__' in globals() else Path(os.getcwd()).parent\n",
    "\n",
    "\n",
    "def _default_cfg():\n",
    "    return {\n",
    "        'defaults': {\n",
    "            'dataset': 'cifar10', 'precision': 'fp32', 'batch': 64,\n",
    "            'warmup': 10, 'runs': 100, 'image_size': 32, 'num_workers': 2,\n",
    "        },\n",
    "        'engines': [\n",
    "            {'engine': 'pytorch', 'providers': ['cpu', 'cuda']},\n",
    "            {'engine': 'onnxruntime', 'providers': ['cpu', 'cuda']},\n",
    "            {'engine': 'openvino', 'providers': ['CPU', 'GPU']},\n",
    "        ],\n",
    "        'models': [\n",
    "            {'name': 'cnn', 'file_onnx': 'cnn_cifar10.onnx', 'file_pt': 'cnn_cifar10.pt'},\n",
    "            {'name': 'mlp', 'file_onnx': 'mlp_cifar10.onnx', 'file_pt': 'mlp_cifar10.pt'},\n",
    "            {'name': 'mobilenetv3', 'file_onnx': 'mobilenetv3_cifar10.onnx', 'file_pt': 'mobilenetv3_cifar10.pt'},\n",
    "            {'name': 'efficientnetlite0', 'file_onnx': 'efficientnetlite0_cifar10.onnx', 'file_pt': 'efficientnetlite0_cifar10.pt'},\n",
    "        ],\n",
    "        'outputs': {\n",
    "            'infer_csv': 'metrics/infer_metrics.csv'\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def load_cfg():\n",
    "    cfg_path = root / 'config/bench_matrix.yaml'\n",
    "    if cfg_path.exists():\n",
    "        try:\n",
    "            return yaml.safe_load(open(cfg_path, 'r', encoding='utf-8'))\n",
    "        except Exception as ex:\n",
    "            logger.warning(\"Failed to read config/bench_matrix.yaml, using defaults: %s\", ex)\n",
    "    return _default_cfg()\n",
    "\n",
    "\n",
    "cfg = load_cfg()\n",
    "defs = cfg['defaults']\n",
    "engines = cfg['engines']\n",
    "models_cfg = {m['name']: m for m in cfg['models']}\n",
    "out_csv = root / cfg['outputs']['infer_csv']\n",
    "onnx_dir = root / 'models_saved/onnx'\n",
    "pt_dir = root / 'models_saved/pytorch'\n",
    "versions = runtime_versions()\n",
    "os_str = versions['os']\n",
    "\n",
    "# Dataset (download used only to match labels; we will preprocess with our function)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "try:\n",
    "    testset = datasets.CIFAR10(root=f'{root}/data', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=defs['batch'], shuffle=False,\n",
    "                                              num_workers=defs['num_workers'])\n",
    "except Exception as ex:\n",
    "    logger.exception(\"Failed to prepare CIFAR10 test loader: %s\", ex)\n",
    "    raise\n",
    "\n",
    "\n",
    "def build_model(name: str):\n",
    "    if name == 'cnn': return CNN()\n",
    "    if name == 'mlp': return MLP(input_size=32 * 32 * 3)\n",
    "    if name == 'mobilenetv3': return MobileNetV3()\n",
    "    if name == 'efficientnetlite0': return EfficientNetLite0()\n",
    "    raise ValueError(f'Unknown model {name}')\n",
    "\n",
    "\n",
    "def bench_pytorch_single_batch(model, xb, yb, device, warmup: int, runs: int):\n",
    "    model = model.to(device).eval()\n",
    "    x_np = preprocess_np(xb.permute(0, 2, 3, 1).numpy())  # ensure NCHW normalized\n",
    "    x_t = torch.from_numpy(x_np).to(device)\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = model(x_t)\n",
    "        if device.type == 'cuda': torch.cuda.synchronize()\n",
    "    # Timed\n",
    "    lat = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _ in range(runs):\n",
    "        t0 = time.perf_counter()\n",
    "        out = model(x_t)\n",
    "        if device.type == 'cuda': torch.cuda.synchronize()\n",
    "        dt = time.perf_counter() - t0\n",
    "        lat.append(dt)\n",
    "        pred = out.argmax(1).detach().cpu().numpy()\n",
    "        correct += int((pred == yb.numpy()).sum())\n",
    "        total += yb.shape[0]\n",
    "    lat_ms = np.array(lat) * 1000.0\n",
    "    lat_ms_mean = float(lat_ms.mean()) if lat_ms.size else 0.0\n",
    "    lat_ms_p95 = float(np.percentile(lat_ms, 95)) if lat_ms.size else 0.0\n",
    "    thr = float(total / sum(lat)) if lat else 0.0\n",
    "    acc = float(correct / total) if total else 0.0\n",
    "    return {\"lat_ms_mean\": lat_ms_mean, \"lat_ms_p95\": lat_ms_p95, \"thr_ips\": thr, \"acc\": acc}\n",
    "\n",
    "\n",
    "def make_ort_session(onnx_path: str, provider: str):\n",
    "    import onnxruntime as ort\n",
    "    providers = [provider]\n",
    "    return ort.InferenceSession(onnx_path, providers=providers)\n",
    "\n",
    "\n",
    "def make_ov_compiled(ir_path: str, device: str):\n",
    "    from openvino import Core\n",
    "    core = Core()\n",
    "    model = core.read_model(ir_path)\n",
    "    return core.compile_model(model, device)\n",
    "\n",
    "\n",
    "def consistency_check(model_name: str, onnx_path: Path, ir_path: Path, xb, device):\n",
    "    # Build single-batch normalized sample\n",
    "    x_np = preprocess_np(xb.permute(0, 2, 3, 1).numpy())\n",
    "    # Torch\n",
    "    model = build_model(model_name)\n",
    "    state = torch.load(pt_dir / models_cfg[model_name]['file_pt'], map_location=str(device))\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    model = model.to(device).eval()\n",
    "    # ORT sess\n",
    "    sess = make_ort_session(str(onnx_path),\n",
    "                            'CUDAExecutionProvider' if device.type == 'cuda' else 'CPUExecutionProvider')\n",
    "    # OV compiled\n",
    "    ov_comp = make_ov_compiled(str(ir_path), 'GPU' if device.type == 'cuda' else 'CPU')\n",
    "    res = compare_logits_torch_ort_ov(model, sess, ov_comp, x_np)\n",
    "    return res\n",
    "\n",
    "\n",
    "# --------------- helpers to reduce bench_once complexity ---------------\n",
    "\n",
    "def _normalize_provider(engine: str, provider: str) -> str:\n",
    "    return provider.upper() if engine in ('pytorch', 'onnxruntime') else provider\n",
    "\n",
    "\n",
    "def _cache_match(model_hash: str, engine: str, provider_csv: str, defs: dict, versions: dict, driver_ver: str) -> dict:\n",
    "    return {\n",
    "        \"model_hash\": model_hash,\n",
    "        \"engine\": engine,\n",
    "        \"provider\": provider_csv,\n",
    "        \"dataset\": defs['dataset'],\n",
    "        \"precision\": defs['precision'],\n",
    "        \"batch\": defs['batch'],\n",
    "        \"warmup\": defs['warmup'],\n",
    "        \"runs\": defs['runs'],\n",
    "        \"torch_ver\": versions['torch_ver'],\n",
    "        \"ort_ver\": versions['ort_ver'],\n",
    "        \"ov_ver\": versions['ov_ver'],\n",
    "        \"driver_ver\": driver_ver,\n",
    "        \"cached\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def _fixed_batch(loader):\n",
    "    xb, yb = next(iter(loader))\n",
    "    x_np = preprocess_np(xb.permute(0, 2, 3, 1).numpy())\n",
    "    return xb, yb, x_np, yb.numpy()\n",
    "\n",
    "\n",
    "def _ensure_ir(model_name: str, onnx_path: Path) -> Path | None:\n",
    "    ir_path = (root / 'models_saved/openvino_ir' / (models_cfg[model_name]['file_onnx'].replace('.onnx', '.xml')))\n",
    "    if ir_path.exists():\n",
    "        return ir_path\n",
    "    try:\n",
    "        logger.info(\"Converting to OpenVINO IR: %s -> %s\", onnx_path, ir_path)\n",
    "        return onnx_to_ir(onnx_path, ir_path.parent, ir_path.name,\n",
    "                          input_shape=(1, 3, defs['image_size'], defs['image_size']))\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"IR conversion failed: %s\", ex)\n",
    "        return None\n",
    "\n",
    "\n",
    "def _row_common(model_name, engine, provider_csv, cached, device_name, cpu_name, gpu_name, os_str, versions,\n",
    "                driver_ver, model_hash):\n",
    "    return {\n",
    "        'ts': utc_timestamp(), 'exp_id': 'engines-bench', 'model': model_name, 'dataset': defs['dataset'],\n",
    "        'precision': defs['precision'], 'engine': engine, 'provider': provider_csv, 'batch': defs['batch'],\n",
    "        'warmup': defs['warmup'], 'runs': defs['runs'], 'cached': cached, 'device_name': device_name,\n",
    "        'cpu_name': cpu_name, 'gpu_name': gpu_name, 'os': os_str, 'torch_ver': versions['torch_ver'],\n",
    "        'ort_ver': versions['ort_ver'], 'ov_ver': versions['ov_ver'], 'driver_ver': driver_ver,\n",
    "        'model_hash': model_hash,\n",
    "    }\n",
    "\n",
    "\n",
    "def _write_cached_row(out_csv, base):\n",
    "    row = {**base,\n",
    "           'lat_ms_mean': '', 'lat_ms_p95': '', 'thr_ips': '', 'acc': '', 'energy_j': 'N/D', 'dt_ms': '', 'avg_power_w': '',\n",
    "           'consistency_ok': '', 'max_abs_diff_torch_ort': '', 'max_abs_diff_torch_ov': '',\n",
    "           'top1_agree_torch_ort': '', 'top1_agree_torch_ov': ''}\n",
    "    csv_append_row(str(out_csv), row, CSV_SCHEMA)\n",
    "\n",
    "\n",
    "def _ov_provider_available(provider: str) -> bool:\n",
    "    try:\n",
    "        return provider in set(ov_available())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _run_pytorch(model_name, onnx_path, xb, yb, x_np, cpu_name, device_pref):\n",
    "    dev = 'cuda' if device_pref.lower() == 'cuda' and torch.cuda.is_available() else 'cpu'\n",
    "    model = build_model(model_name)\n",
    "    state = torch.load(pt_dir / models_cfg[model_name]['file_pt'], map_location=dev)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    device = torch.device(dev)\n",
    "    # Energy\n",
    "    meter = GpuEnergyMeterNVML(0) if (device.type == 'cuda') else None\n",
    "    dt_ms = ''\n",
    "    avg_power_w = ''\n",
    "    if meter:\n",
    "        x_t = torch.from_numpy(x_np).to(device)\n",
    "        model = model.to(device).eval()\n",
    "        local_runs = 300 if model_name in ('cnn', 'mlp') else 100\n",
    "        def _one():\n",
    "            for _ in range(local_runs):\n",
    "                _ = model(x_t)\n",
    "                torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        e_j, dt_ms_val = meter.measure(_one)\n",
    "        dt_ms = float(dt_ms_val)\n",
    "        avg_power_w = (float(e_j) / (dt_ms / 1000.0)) if (isinstance(e_j, (int, float)) and dt_ms > 0) else ''\n",
    "        e_j = float(e_j)\n",
    "    else:\n",
    "        e_j = -1.0\n",
    "    metrics = bench_pytorch_single_batch(model, xb, yb, device=device, warmup=defs['warmup'], runs=defs['runs'])\n",
    "    energy_val = e_j if e_j >= 0 else 'N/D'\n",
    "    # Consistency\n",
    "    ir_path = _ensure_ir(model_name, onnx_path)\n",
    "    consistency = {'consistency_ok': False, 'max_abs_diff_torch_ort': '', 'max_abs_diff_torch_ov': '',\n",
    "                   'top1_agree_torch_ort': '', 'top1_agree_torch_ov': ''}\n",
    "    if ir_path is not None and ir_path.exists():\n",
    "        try:\n",
    "            consistency = consistency_check(model_name, onnx_path, ir_path, xb, device)\n",
    "        except Exception as ex:\n",
    "            logger.warning(\"Consistency check failed: %s\", ex)\n",
    "    device_name = (torch.cuda.get_device_name(0) if device.type == 'cuda' else cpu_name)\n",
    "    return {\"status\": \"ok\", \"metrics\": metrics, \"energy_j\": energy_val, \"dt_ms\": dt_ms, \"avg_power_w\": avg_power_w, \"device_name\": device_name,\n",
    "            \"consistency\": consistency}\n",
    "\n",
    "\n",
    "def _run_ort(onnx_path, x_np, provider, y, cpu_name):\n",
    "    prov = 'CUDAExecutionProvider' if provider.lower() == 'cuda' else 'CPUExecutionProvider'\n",
    "    # Energy\n",
    "    meter = GpuEnergyMeterNVML(0) if (prov == 'CUDAExecutionProvider' and torch.cuda.is_available()) else None\n",
    "    dt_ms = ''\n",
    "    avg_power_w = ''\n",
    "    if meter:\n",
    "        sess = make_ort_session(str(onnx_path), prov)\n",
    "        inp_name = sess.get_inputs()[0].name\n",
    "        basename = os.path.basename(onnx_path).lower()\n",
    "        local_runs = 300 if any(k in basename for k in ('cnn', 'mlp')) else 100\n",
    "        def _one_inf():\n",
    "            for _ in range(local_runs):\n",
    "                _ = sess.run(None, {inp_name: x_np})\n",
    "        e_j, dt_ms_val = meter.measure(_one_inf)\n",
    "        dt_ms = float(dt_ms_val)\n",
    "        avg_power_w = (float(e_j) / (dt_ms / 1000.0)) if (isinstance(e_j, (int, float)) and dt_ms > 0) else ''\n",
    "        e_j = float(e_j)\n",
    "    else:\n",
    "        e_j = -1.0\n",
    "    # Metrics\n",
    "    try:\n",
    "        metrics = ort_bench(str(onnx_path), x_np, provider=prov, warmup=defs['warmup'], runs=defs['runs'], y_true=y)\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"ORT bench failed: %s\", ex)\n",
    "        metrics = {\"lat_ms_mean\": '', \"lat_ms_p95\": '', \"thr_ips\": '', \"acc\": ''}\n",
    "    device_name = (torch.cuda.get_device_name(0) if provider.lower() == 'cuda' else cpu_name)\n",
    "    return {\"status\": \"ok\", \"metrics\": metrics, \"energy_j\": (e_j if e_j >= 0 else 'N/D'), \"dt_ms\": dt_ms, \"avg_power_w\": avg_power_w,\n",
    "            \"device_name\": device_name, \"consistency\": None}\n",
    "\n",
    "\n",
    "def _run_ov(model_name, onnx_path, x_np, provider, y):\n",
    "    if not _ov_provider_available(provider):\n",
    "        return {\"status\": \"unavailable\"}\n",
    "    ir_path = _ensure_ir(model_name, onnx_path)\n",
    "    if ir_path is None:\n",
    "        return {\"status\": \"missing_ir\"}\n",
    "    metrics = ov_bench(str(ir_path), x_np, device=provider, warmup=defs['warmup'], runs=defs['runs'], y_true=y)\n",
    "    return {\"status\": \"ok\", \"metrics\": metrics, \"energy_j\": 'N/D', \"dt_ms\": '', \"avg_power_w\": '', \"device_name\": provider, \"consistency\": None}\n",
    "\n",
    "\n",
    "def _exec_engine(model_name, engine, provider, onnx_path, xb, yb, x_np, y, cpu_name):\n",
    "    if engine == 'pytorch':\n",
    "        return _run_pytorch(model_name, onnx_path, xb, yb, x_np, cpu_name, provider)\n",
    "    if engine == 'onnxruntime':\n",
    "        return _run_ort(onnx_path, x_np, provider, y, cpu_name)\n",
    "    if engine == 'openvino':\n",
    "        return _run_ov(model_name, onnx_path, x_np, provider, y)\n",
    "    return {\"status\": \"error\", \"error\": f\"Unknown engine {engine}\"}\n",
    "\n",
    "\n",
    "# --------------------------- minimized bench_once ---------------------------\n",
    "\n",
    "def bench_once(model_name: str, engine: str, provider: str):\n",
    "    onnx_path = onnx_dir / models_cfg[model_name]['file_onnx']\n",
    "    model_hash = sha256_file(str(onnx_path))\n",
    "    gpu_name, driver_ver = get_gpu_name_and_driver()\n",
    "    cpu_name = get_cpu_name()\n",
    "    provider_csv = _normalize_provider(engine, provider)\n",
    "\n",
    "    # Cache\n",
    "    match = _cache_match(model_hash, engine, provider_csv, defs, versions, driver_ver)\n",
    "    if csv_has_row_with(str(out_csv), match):\n",
    "        base = _row_common(model_name, engine, provider_csv, True, provider_csv, cpu_name, gpu_name, os_str, versions,\n",
    "                           driver_ver, model_hash)\n",
    "        _write_cached_row(out_csv, base)\n",
    "        return 'cached'\n",
    "\n",
    "    # Fixed batch\n",
    "    xb, yb, x_np, y = _fixed_batch(test_loader)\n",
    "\n",
    "    # Execute\n",
    "    res = _exec_engine(model_name, engine, provider, onnx_path, xb, yb, x_np, y, cpu_name)\n",
    "    status = res.get('status')\n",
    "\n",
    "    # Handle non-ok statuses (write informative rows)\n",
    "    if status in ('unavailable', 'missing_ir', 'error'):\n",
    "        device_name = provider_csv\n",
    "        base = _row_common(model_name, engine, provider_csv, False, device_name, cpu_name, gpu_name, os_str, versions,\n",
    "                           driver_ver, model_hash)\n",
    "        row = {**base, 'lat_ms_mean': '', 'lat_ms_p95': '', 'thr_ips': '', 'acc': '', 'energy_j': 'N/D', 'dt_ms': '', 'avg_power_w': '',\n",
    "               'consistency_ok': False, 'max_abs_diff_torch_ort': '', 'max_abs_diff_torch_ov': '',\n",
    "               'top1_agree_torch_ort': '', 'top1_agree_torch_ov': ''}\n",
    "        csv_append_row(str(out_csv), row, CSV_SCHEMA)\n",
    "        return status\n",
    "\n",
    "    # OK path: write results\n",
    "    metrics = res['metrics']\n",
    "    energy_j = res['energy_j']\n",
    "    dt_ms = res.get('dt_ms', '')\n",
    "    avg_power_w = res.get('avg_power_w', '')\n",
    "    device_name = res['device_name']\n",
    "    base = _row_common(model_name, engine, provider_csv, False, device_name, cpu_name, gpu_name, os_str, versions,\n",
    "                       driver_ver, model_hash)\n",
    "    if engine == 'pytorch':\n",
    "        cons = res.get('consistency', {}) or {}\n",
    "        row = {**base, 'lat_ms_mean': metrics['lat_ms_mean'], 'lat_ms_p95': metrics['lat_ms_p95'],\n",
    "               'thr_ips': metrics['thr_ips'], 'acc': metrics['acc'], 'energy_j': energy_j, 'dt_ms': dt_ms, 'avg_power_w': avg_power_w,\n",
    "               'consistency_ok': cons.get('consistency_ok', False),\n",
    "               'max_abs_diff_torch_ort': cons.get('max_abs_diff_torch_ort', ''),\n",
    "               'max_abs_diff_torch_ov': cons.get('max_abs_diff_torch_ov', ''),\n",
    "               'top1_agree_torch_ort': cons.get('top1_agree_torch_ort', ''),\n",
    "               'top1_agree_torch_ov': cons.get('top1_agree_torch_ov', '')}\n",
    "    else:\n",
    "        row = {**base, 'lat_ms_mean': metrics['lat_ms_mean'], 'lat_ms_p95': metrics['lat_ms_p95'],\n",
    "               'thr_ips': metrics['thr_ips'], 'acc': metrics['acc'], 'energy_j': energy_j, 'dt_ms': dt_ms, 'avg_power_w': avg_power_w,\n",
    "               'consistency_ok': '', 'max_abs_diff_torch_ort': '', 'max_abs_diff_torch_ov': '',\n",
    "               'top1_agree_torch_ort': '', 'top1_agree_torch_ov': ''}\n",
    "    csv_append_row(str(out_csv), row, CSV_SCHEMA)\n",
    "    return 'done'\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for m in models_cfg.keys():\n",
    "    for e in engines:\n",
    "        eng = e['engine']\n",
    "        for prov in e['providers']:\n",
    "            logger.info(\"Benchmark: model=%s engine=%s provider=%s\", m, eng, prov)\n",
    "            try:\n",
    "                _ = bench_once(m, eng, prov)\n",
    "            except Exception as ex:\n",
    "                logger.exception(\"Error in bench_once: %s\", ex)\n",
    "print('Benchmark completed.')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 20:40:36 | INFO | nb02 | Benchmark: model=cnn engine=pytorch provider=cpu\n",
      "2025-08-13 20:40:43 | INFO | nb02 | Benchmark: model=cnn engine=pytorch provider=cuda\n",
      "2025-08-13 20:40:50 | INFO | energy | gpu-energy: method=nvml_total_energy energy_j=17.271000 dt_ms=371.935 avg_power_w=46.435\n",
      "2025-08-13 20:40:51 | INFO | nb02 | Benchmark: model=cnn engine=onnxruntime provider=cpu\n",
      "2025-08-13 20:40:57 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\cnn_cifar10.onnx | provider=CPUExecutionProvider\n",
      "2025-08-13 20:40:58 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 2.6378349996957695, 'lat_ms_p95': 3.311195000605948, 'thr_ips': 24262.32118664789, 'acc': 0.828125}\n",
      "2025-08-13 20:40:58 | INFO | nb02 | Benchmark: model=cnn engine=onnxruntime provider=cuda\n",
      "2025-08-13 20:41:04 | INFO | energy | gpu-energy: method=nvml_total_energy energy_j=14.210000 dt_ms=555.385 avg_power_w=25.586\n",
      "2025-08-13 20:41:04 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\cnn_cifar10.onnx | provider=CUDAExecutionProvider\n",
      "2025-08-13 20:41:04 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 0.6289610002204427, 'lat_ms_p95': 0.855420001062157, 'thr_ips': 101755.11673628227, 'acc': 0.828125}\n",
      "2025-08-13 20:41:04 | INFO | nb02 | Benchmark: model=cnn engine=openvino provider=CPU\n",
      "2025-08-13 20:41:11 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\cnn_cifar10.xml\n",
      "2025-08-13 20:41:11 | INFO | infer_openvino | Compiling model for device: CPU\n",
      "2025-08-13 20:41:12 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 4.045943000019179, 'lat_ms_p95': 4.894950001289543, 'thr_ips': 15818.314790815546, 'acc': 0.828125}\n",
      "2025-08-13 20:41:12 | INFO | nb02 | Benchmark: model=cnn engine=openvino provider=GPU\n",
      "2025-08-13 20:41:18 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\cnn_cifar10.xml\n",
      "2025-08-13 20:41:18 | INFO | infer_openvino | Compiling model for device: GPU\n",
      "2025-08-13 20:41:19 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 1.0208390001571388, 'lat_ms_p95': 1.4950299988413462, 'thr_ips': 62693.529528308005, 'acc': 0.828125}\n",
      "2025-08-13 20:41:19 | INFO | nb02 | Benchmark: model=cnn engine=openvino provider=NPU\n",
      "2025-08-13 20:41:26 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\cnn_cifar10.xml\n",
      "2025-08-13 20:41:26 | INFO | infer_openvino | Compiling model for device: NPU\n",
      "2025-08-13 20:41:26 | WARNING | infer_openvino | NPU compile with precision hint failed; falling back to default.\n",
      "2025-08-13 20:41:28 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 0.3819064218868107, 'lat_ms_p95': 0.46390999996219745, 'thr_ips': 2618.4424840501365, 'acc': 0.828125}\n",
      "2025-08-13 20:41:28 | INFO | nb02 | Benchmark: model=mlp engine=pytorch provider=cpu\n",
      "2025-08-13 20:41:35 | INFO | nb02 | Benchmark: model=mlp engine=pytorch provider=cuda\n",
      "2025-08-13 20:41:41 | INFO | energy | gpu-energy: method=nvml_total_energy energy_j=8.883000 dt_ms=168.321 avg_power_w=52.774\n",
      "2025-08-13 20:41:42 | INFO | nb02 | Benchmark: model=mlp engine=onnxruntime provider=cpu\n",
      "2025-08-13 20:41:49 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mlp_cifar10.onnx | provider=CPUExecutionProvider\n",
      "2025-08-13 20:41:49 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 0.8203210001374828, 'lat_ms_p95': 0.6383750011082152, 'thr_ips': 78018.23918840775, 'acc': 0.546875}\n",
      "2025-08-13 20:41:49 | INFO | nb02 | Benchmark: model=mlp engine=onnxruntime provider=cuda\n",
      "2025-08-13 20:41:55 | INFO | energy | gpu-energy: method=nvml_total_energy energy_j=9.352000 dt_ms=400.316 avg_power_w=23.362\n",
      "2025-08-13 20:41:55 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mlp_cifar10.onnx | provider=CUDAExecutionProvider\n",
      "2025-08-13 20:41:55 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 0.46289199988677865, 'lat_ms_p95': 0.7406399981846334, 'thr_ips': 138261.1927094314, 'acc': 0.546875}\n",
      "2025-08-13 20:41:55 | INFO | nb02 | Benchmark: model=mlp engine=openvino provider=CPU\n",
      "2025-08-13 20:42:02 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mlp_cifar10.xml\n",
      "2025-08-13 20:42:02 | INFO | infer_openvino | Compiling model for device: CPU\n",
      "2025-08-13 20:42:02 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 0.570808000084071, 'lat_ms_p95': 0.7084350008881302, 'thr_ips': 112121.7642194465, 'acc': 0.546875}\n",
      "2025-08-13 20:42:02 | INFO | nb02 | Benchmark: model=mlp engine=openvino provider=GPU\n",
      "2025-08-13 20:42:08 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mlp_cifar10.xml\n",
      "2025-08-13 20:42:08 | INFO | infer_openvino | Compiling model for device: GPU\n",
      "2025-08-13 20:42:10 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 0.7873930001733243, 'lat_ms_p95': 1.089094999770168, 'thr_ips': 81280.88513094738, 'acc': 0.546875}\n",
      "2025-08-13 20:42:10 | INFO | nb02 | Benchmark: model=mlp engine=openvino provider=NPU\n",
      "2025-08-13 20:42:16 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mlp_cifar10.xml\n",
      "2025-08-13 20:42:16 | INFO | infer_openvino | Compiling model for device: NPU\n",
      "2025-08-13 20:42:16 | WARNING | infer_openvino | NPU compile with precision hint failed; falling back to default.\n",
      "2025-08-13 20:42:19 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 0.42337560936459795, 'lat_ms_p95': 0.5731100018238066, 'thr_ips': 2361.96884723898, 'acc': 0.546875}\n",
      "2025-08-13 20:42:19 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=pytorch provider=cpu\n",
      "2025-08-13 20:42:29 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=pytorch provider=cuda\n",
      "2025-08-13 20:42:37 | INFO | energy | gpu-energy: method=nvml_total_energy energy_j=27.414000 dt_ms=941.767 avg_power_w=29.109\n",
      "2025-08-13 20:42:40 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=onnxruntime provider=cpu\n",
      "2025-08-13 20:42:46 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mobilenetv3_cifar10.onnx | provider=CPUExecutionProvider\n",
      "2025-08-13 20:42:47 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 5.942589000042062, 'lat_ms_p95': 6.42222499827767, 'thr_ips': 10769.716700843184, 'acc': 0.78125}\n",
      "2025-08-13 20:42:47 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=onnxruntime provider=cuda\n",
      "2025-08-13 20:42:53 | INFO | energy | gpu-energy: method=nvml_total_energy energy_j=19.443000 dt_ms=472.966 avg_power_w=41.109\n",
      "2025-08-13 20:42:53 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mobilenetv3_cifar10.onnx | provider=CUDAExecutionProvider\n",
      "2025-08-13 20:42:54 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 2.5256389996866346, 'lat_ms_p95': 3.212450001046818, 'thr_ips': 25340.121849536183, 'acc': 0.78125}\n",
      "2025-08-13 20:42:54 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=openvino provider=CPU\n",
      "2025-08-13 20:43:00 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mobilenetv3_cifar10.xml\n",
      "2025-08-13 20:43:00 | INFO | infer_openvino | Compiling model for device: CPU\n",
      "2025-08-13 20:43:01 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 7.7177530000699335, 'lat_ms_p95': 9.422255001118174, 'thr_ips': 8292.56909354576, 'acc': 0.78125}\n",
      "2025-08-13 20:43:01 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=openvino provider=GPU\n",
      "2025-08-13 20:43:07 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mobilenetv3_cifar10.xml\n",
      "2025-08-13 20:43:07 | INFO | infer_openvino | Compiling model for device: GPU\n",
      "2025-08-13 20:43:09 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 3.734628999918641, 'lat_ms_p95': 4.157819999818457, 'thr_ips': 17136.90971750989, 'acc': 0.78125}\n",
      "2025-08-13 20:43:09 | INFO | nb02 | Benchmark: model=mobilenetv3 engine=openvino provider=NPU\n",
      "2025-08-13 20:43:15 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\mobilenetv3_cifar10.xml\n",
      "2025-08-13 20:43:15 | INFO | infer_openvino | Compiling model for device: NPU\n",
      "2025-08-13 20:43:15 | WARNING | infer_openvino | NPU compile with precision hint failed; falling back to default.\n",
      "2025-08-13 20:43:21 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 0.8078273906261302, 'lat_ms_p95': 0.9069050005564349, 'thr_ips': 1237.8882068172024, 'acc': 0.78125}\n",
      "2025-08-13 20:43:21 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=pytorch provider=cpu\n",
      "2025-08-13 20:43:37 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=pytorch provider=cuda\n",
      "2025-08-13 20:43:45 | INFO | energy | gpu-energy: method=nvml_total_energy energy_j=62.679000 dt_ms=1452.691 avg_power_w=43.147\n",
      "2025-08-13 20:43:48 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=onnxruntime provider=cpu\n",
      "2025-08-13 20:43:54 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\efficientnetlite0_cifar10.onnx | provider=CPUExecutionProvider\n",
      "2025-08-13 20:43:57 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 24.536618000092858, 'lat_ms_p95': 26.267550000738993, 'thr_ips': 2608.346431433941, 'acc': 0.859375}\n",
      "2025-08-13 20:43:57 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=onnxruntime provider=cuda\n",
      "2025-08-13 20:44:04 | INFO | energy | gpu-energy: method=nvml_total_energy energy_j=26.319000 dt_ms=762.975 avg_power_w=34.495\n",
      "2025-08-13 20:44:04 | INFO | infer_ort | Creating ORT session: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\efficientnetlite0_cifar10.onnx | provider=CUDAExecutionProvider\n",
      "2025-08-13 20:44:05 | INFO | infer_ort | ORT metrics: {'lat_ms_mean': 4.791040999916731, 'lat_ms_p95': 5.329109999911452, 'thr_ips': 13358.265980423112, 'acc': 0.859375}\n",
      "2025-08-13 20:44:05 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=openvino provider=CPU\n",
      "2025-08-13 20:44:11 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\efficientnetlite0_cifar10.xml\n",
      "2025-08-13 20:44:11 | INFO | infer_openvino | Compiling model for device: CPU\n",
      "2025-08-13 20:44:15 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 35.1577589999215, 'lat_ms_p95': 38.10090499737271, 'thr_ips': 1820.3663094721965, 'acc': 0.859375}\n",
      "2025-08-13 20:44:15 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=openvino provider=GPU\n",
      "2025-08-13 20:44:23 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\efficientnetlite0_cifar10.xml\n",
      "2025-08-13 20:44:23 | INFO | infer_openvino | Compiling model for device: GPU\n",
      "2025-08-13 20:44:27 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 14.036970999950427, 'lat_ms_p95': 15.680520000205433, 'thr_ips': 4559.388204209157, 'acc': 0.859375}\n",
      "2025-08-13 20:44:27 | INFO | nb02 | Benchmark: model=efficientnetlite0 engine=openvino provider=NPU\n",
      "2025-08-13 20:44:33 | INFO | infer_openvino | Reading IR: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\openvino_ir\\efficientnetlite0_cifar10.xml\n",
      "2025-08-13 20:44:33 | INFO | infer_openvino | Compiling model for device: NPU\n",
      "2025-08-13 20:44:33 | WARNING | infer_openvino | NPU compile with precision hint failed; falling back to default.\n",
      "2025-08-13 20:44:45 | INFO | infer_openvino | OV metrics: {'lat_ms_mean': 1.7869356406202996, 'lat_ms_p95': 2.961294998385706, 'thr_ips': 559.6172448901794, 'acc': 0.859375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark completed.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c56de0b06652fcc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T18:44:46.184763Z",
     "start_time": "2025-08-13T18:44:46.073109Z"
    }
   },
   "source": [
    "# Summarize energy rows into a dedicated CSV for plots\n",
    "try:\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(out_csv)\n",
    "    energy_csv = root / 'metrics' / 'inference_energy_summary.csv'\n",
    "    from utils.io import ensure_dir\n",
    "\n",
    "    ensure_dir(str(energy_csv.parent))\n",
    "    cols = ['model', 'engine', 'provider', 'batch', 'runs', 'energy_j', 'device_name', 'gpu_name', 'cpu_name', 'os']\n",
    "    df_energy = df[df['energy_j'].astype(str) != 'N/D'][cols]\n",
    "    df_energy.to_csv(energy_csv, index=False)\n",
    "    logger.info(\"Wrote energy summary: %s (rows=%d)\", energy_csv, len(df_energy))\n",
    "except Exception as ex:\n",
    "    logger.warning(\"Energy summary not generated: %s\", ex)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 20:44:46 | INFO | nb02 | Wrote energy summary: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\metrics\\inference_energy_summary.csv (rows=8)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ed8558cfc7cc2fad",
   "metadata": {},
   "source": [
    "## Notes (Windows assumptions)\n",
    "\n",
    "- No CPU energy via RAPL/PCM on Windows; energy_j is measured only on NVIDIA GPUs via NVML. Others are marked as \"N/D\".\n",
    "- NPU is optional; if not available, rows are recorded with provider status unavailable/missing.\n",
    "- This notebook writes a single CSV and prints sanity tables only; all plots are produced in `04_results_and_plots.ipynb`.\n",
    "\n",
    "Further reading: see [results](../docs/results.md) for interpreting metrics, [energy](../docs/energy.md) for measurement details, and [troubleshooting](../docs/troubleshooting.md) for common issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
