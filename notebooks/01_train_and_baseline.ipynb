{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ed775b",
   "metadata": {},
   "source": [
    "# 01 Â· Training and Baselines (PyTorch)\n",
    "\n",
    "Train baseline models on CIFAR-10, export ONNX artifacts, and log per-epoch metrics for reproducible comparisons.\n",
    "\n",
    "- Methodology and reproducibility: docs/reproducibility.md\n",
    "- Benchmark protocol and metrics: docs/benchmarks.md\n",
    "- Where results go: docs/results.md"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Cache policy:\n",
    " >\n",
    "> - This notebook reuses existing artifacts under `models_saved/` when found:\n",
    " >   - If `.pt` weights and `.pkl` history exist for a model, training is skipped unless `FORCE_RETRAIN=True`.\n",
    " >   - If the `.onnx` file exists, export is skipped unless `FORCE_REEXPORT_ONNX=True`.\n",
    " > - Datasets are stored in `ROOT/data`. If present, they are reused; otherwise, they are downloaded once.\n",
    " >\n",
    " > You can change force flags at the top of the code cell below."
   ],
   "id": "3e9ae321b7f5eef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Setup & Logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "from utils.logging_utils import get_logger\n",
    "\n",
    "os.environ.setdefault(\"LOG_LEVEL\", \"INFO\")\n",
    "root = Path(__file__).resolve().parent.parent if '__file__' in globals() else Path(os.getcwd()).parent\n",
    "logger = get_logger(\"nb01\")\n",
    "logger.info(\"Notebook 01 starting. Root=%s\", root)\n",
    "\n",
    "# Load config and summarize cache\n",
    "cfg = yaml.safe_load(open(root / 'config/bench_matrix.yaml', 'r', encoding='utf-8'))\n",
    "models_cfg = {m['name']: m for m in cfg['models']}\n",
    "\n",
    "pt_dir = root / 'models_saved' / 'pytorch'\n",
    "onnx_dir = root / 'models_saved' / 'onnx'\n",
    "\n",
    "rows = []\n",
    "for name, c in models_cfg.items():\n",
    "    pt = pt_dir / c['file_pt']\n",
    "    hist = pt_dir / c['file_hist']\n",
    "    onnx = onnx_dir / c['file_onnx']\n",
    "    rows.append((name, pt.exists(), hist.exists(), onnx.exists()))\n",
    "\n",
    "print(\"Model cache status (PT/HIST/ONNX):\")\n",
    "for name, pt_ok, hist_ok, onnx_ok in rows:\n",
    "    print(f\"- {name}: pt={pt_ok}, hist={hist_ok}, onnx={onnx_ok}\")\n",
    "logger.info(\"Cache status listed for %d models\", len(rows))\n",
    "print(\"Use FORCE_RETRAIN=1 and/or FORCE_REEXPORT_ONNX=1 to override cache reuse.\")"
   ],
   "id": "1616e30a5a2a8879"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7959ae4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:27:10.922617Z",
     "start_time": "2025-08-13T08:57:20.972156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 10:57:34 | INFO | nb01 | Config loaded. batch=64, epochs=20, seed=42\n",
      "INFO:utils.data_utils:Dataset CIFAR-10: using cached data at C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\data\n",
      "2025-08-13 10:57:35 | INFO | nb01 | [train] Training cnn on cuda with input shape [3, 32, 32]\n",
      "2025-08-13 10:57:48 | INFO | train_utils | Epoch 1/20 | Loss: 1.4993 | Acc: 0.4533 | Time: 12.3s\n",
      "2025-08-13 10:57:59 | INFO | train_utils | Epoch 2/20 | Loss: 1.1713 | Acc: 0.5813 | Time: 11.8s\n",
      "2025-08-13 10:58:11 | INFO | train_utils | Epoch 3/20 | Loss: 1.0425 | Acc: 0.6286 | Time: 11.7s\n",
      "2025-08-13 10:58:23 | INFO | train_utils | Epoch 4/20 | Loss: 0.9723 | Acc: 0.6578 | Time: 11.9s\n",
      "2025-08-13 10:58:35 | INFO | train_utils | Epoch 5/20 | Loss: 0.9042 | Acc: 0.6834 | Time: 11.5s\n",
      "2025-08-13 10:58:47 | INFO | train_utils | Epoch 6/20 | Loss: 0.8544 | Acc: 0.7010 | Time: 12.0s\n",
      "2025-08-13 10:58:58 | INFO | train_utils | Epoch 7/20 | Loss: 0.8173 | Acc: 0.7177 | Time: 11.4s\n",
      "2025-08-13 10:59:09 | INFO | train_utils | Epoch 8/20 | Loss: 0.7848 | Acc: 0.7263 | Time: 11.6s\n",
      "2025-08-13 10:59:23 | INFO | train_utils | Epoch 9/20 | Loss: 0.7577 | Acc: 0.7390 | Time: 13.2s\n",
      "2025-08-13 10:59:39 | INFO | train_utils | Epoch 10/20 | Loss: 0.7286 | Acc: 0.7464 | Time: 16.4s\n",
      "2025-08-13 10:59:51 | INFO | train_utils | Epoch 11/20 | Loss: 0.7073 | Acc: 0.7560 | Time: 11.8s\n",
      "2025-08-13 11:00:03 | INFO | train_utils | Epoch 12/20 | Loss: 0.6901 | Acc: 0.7627 | Time: 11.6s\n",
      "2025-08-13 11:00:14 | INFO | train_utils | Epoch 13/20 | Loss: 0.6704 | Acc: 0.7685 | Time: 11.6s\n",
      "2025-08-13 11:00:27 | INFO | train_utils | Epoch 14/20 | Loss: 0.6590 | Acc: 0.7727 | Time: 12.6s\n",
      "2025-08-13 11:00:38 | INFO | train_utils | Epoch 15/20 | Loss: 0.6456 | Acc: 0.7759 | Time: 11.5s\n",
      "2025-08-13 11:00:50 | INFO | train_utils | Epoch 16/20 | Loss: 0.6287 | Acc: 0.7827 | Time: 11.8s\n",
      "2025-08-13 11:01:02 | INFO | train_utils | Epoch 17/20 | Loss: 0.6193 | Acc: 0.7852 | Time: 11.6s\n",
      "2025-08-13 11:01:13 | INFO | train_utils | Epoch 18/20 | Loss: 0.6091 | Acc: 0.7915 | Time: 11.5s\n",
      "2025-08-13 11:01:25 | INFO | train_utils | Epoch 19/20 | Loss: 0.5984 | Acc: 0.7960 | Time: 11.6s\n",
      "2025-08-13 11:01:37 | INFO | train_utils | Epoch 20/20 | Loss: 0.5842 | Acc: 0.8001 | Time: 12.2s\n",
      "2025-08-13 11:01:37 | INFO | train_utils | Training completed in 241.7s\n",
      "2025-08-13 11:01:37 | INFO | nb01 | Saved epoch history: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\metrics\\cnn_train_history.csv\n",
      "2025-08-13 11:01:37 | INFO | nb01 | [save] Wrote C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\pytorch\\cnn_cifar10.pt and C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\pytorch\\cnn_cifar10_hist.pkl\n",
      "2025-08-13 11:01:37 | INFO | nb01 | [save] Wrote C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\cnn_cifar10.onnx\n",
      "2025-08-13 11:02:01 | INFO | nb01 | [train] Training mlp on cuda with input shape [3, 32, 32]\n",
      "2025-08-13 11:02:13 | INFO | train_utils | Epoch 1/20 | Loss: 1.8663 | Acc: 0.3237 | Time: 11.2s\n",
      "2025-08-13 11:02:24 | INFO | train_utils | Epoch 2/20 | Loss: 1.6905 | Acc: 0.3919 | Time: 11.1s\n",
      "2025-08-13 11:02:35 | INFO | train_utils | Epoch 3/20 | Loss: 1.6252 | Acc: 0.4151 | Time: 11.2s\n",
      "2025-08-13 11:02:46 | INFO | train_utils | Epoch 4/20 | Loss: 1.5812 | Acc: 0.4308 | Time: 11.3s\n",
      "2025-08-13 11:02:58 | INFO | train_utils | Epoch 5/20 | Loss: 1.5522 | Acc: 0.4394 | Time: 11.4s\n",
      "2025-08-13 11:03:14 | INFO | train_utils | Epoch 6/20 | Loss: 1.5275 | Acc: 0.4517 | Time: 15.8s\n",
      "2025-08-13 11:03:25 | INFO | train_utils | Epoch 7/20 | Loss: 1.5061 | Acc: 0.4598 | Time: 11.6s\n",
      "2025-08-13 11:03:37 | INFO | train_utils | Epoch 8/20 | Loss: 1.4863 | Acc: 0.4632 | Time: 11.6s\n",
      "2025-08-13 11:03:48 | INFO | train_utils | Epoch 9/20 | Loss: 1.4738 | Acc: 0.4700 | Time: 11.4s\n",
      "2025-08-13 11:04:00 | INFO | train_utils | Epoch 10/20 | Loss: 1.4527 | Acc: 0.4770 | Time: 12.1s\n",
      "2025-08-13 11:04:12 | INFO | train_utils | Epoch 11/20 | Loss: 1.4474 | Acc: 0.4797 | Time: 11.2s\n",
      "2025-08-13 11:04:23 | INFO | train_utils | Epoch 12/20 | Loss: 1.4314 | Acc: 0.4886 | Time: 11.9s\n",
      "2025-08-13 11:04:35 | INFO | train_utils | Epoch 13/20 | Loss: 1.4206 | Acc: 0.4915 | Time: 11.4s\n",
      "2025-08-13 11:04:46 | INFO | train_utils | Epoch 14/20 | Loss: 1.4130 | Acc: 0.4927 | Time: 11.5s\n",
      "2025-08-13 11:04:58 | INFO | train_utils | Epoch 15/20 | Loss: 1.4019 | Acc: 0.4958 | Time: 11.2s\n",
      "2025-08-13 11:05:09 | INFO | train_utils | Epoch 16/20 | Loss: 1.3989 | Acc: 0.4986 | Time: 11.4s\n",
      "2025-08-13 11:05:21 | INFO | train_utils | Epoch 17/20 | Loss: 1.3786 | Acc: 0.5070 | Time: 11.6s\n",
      "2025-08-13 11:05:32 | INFO | train_utils | Epoch 18/20 | Loss: 1.3760 | Acc: 0.5065 | Time: 11.3s\n",
      "2025-08-13 11:05:43 | INFO | train_utils | Epoch 19/20 | Loss: 1.3682 | Acc: 0.5100 | Time: 11.2s\n",
      "2025-08-13 11:05:54 | INFO | train_utils | Epoch 20/20 | Loss: 1.3633 | Acc: 0.5087 | Time: 11.1s\n",
      "2025-08-13 11:05:54 | INFO | train_utils | Training completed in 232.8s\n",
      "2025-08-13 11:05:54 | INFO | nb01 | Saved epoch history: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\metrics\\mlp_train_history.csv\n",
      "2025-08-13 11:05:54 | INFO | nb01 | [save] Wrote C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\pytorch\\mlp_cifar10.pt and C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\pytorch\\mlp_cifar10_hist.pkl\n",
      "2025-08-13 11:05:54 | INFO | nb01 | [save] Wrote C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mlp_cifar10.onnx\n",
      "2025-08-13 11:06:18 | INFO | nb01 | [train] Training mobilenetv3 on cuda with input shape [3, 32, 32]\n",
      "2025-08-13 11:06:40 | INFO | train_utils | Epoch 1/20 | Loss: 1.6728 | Acc: 0.3774 | Time: 22.2s\n",
      "2025-08-13 11:07:02 | INFO | train_utils | Epoch 2/20 | Loss: 1.3608 | Acc: 0.5030 | Time: 22.2s\n",
      "2025-08-13 11:07:25 | INFO | train_utils | Epoch 3/20 | Loss: 1.1927 | Acc: 0.5725 | Time: 23.1s\n",
      "2025-08-13 11:07:49 | INFO | train_utils | Epoch 4/20 | Loss: 1.0664 | Acc: 0.6221 | Time: 23.9s\n",
      "2025-08-13 11:08:12 | INFO | train_utils | Epoch 5/20 | Loss: 0.9767 | Acc: 0.6554 | Time: 23.2s\n",
      "2025-08-13 11:08:37 | INFO | train_utils | Epoch 6/20 | Loss: 0.9168 | Acc: 0.6781 | Time: 24.4s\n",
      "2025-08-13 11:09:00 | INFO | train_utils | Epoch 7/20 | Loss: 0.8566 | Acc: 0.7009 | Time: 23.8s\n",
      "2025-08-13 11:09:24 | INFO | train_utils | Epoch 8/20 | Loss: 0.8148 | Acc: 0.7148 | Time: 23.9s\n",
      "2025-08-13 11:09:49 | INFO | train_utils | Epoch 9/20 | Loss: 0.7914 | Acc: 0.7252 | Time: 24.5s\n",
      "2025-08-13 11:10:12 | INFO | train_utils | Epoch 10/20 | Loss: 0.7540 | Acc: 0.7378 | Time: 23.4s\n",
      "2025-08-13 11:10:38 | INFO | train_utils | Epoch 11/20 | Loss: 0.7220 | Acc: 0.7495 | Time: 25.3s\n",
      "2025-08-13 11:11:02 | INFO | train_utils | Epoch 12/20 | Loss: 0.7016 | Acc: 0.7572 | Time: 24.0s\n",
      "2025-08-13 11:11:25 | INFO | train_utils | Epoch 13/20 | Loss: 0.6892 | Acc: 0.7606 | Time: 24.0s\n",
      "2025-08-13 11:11:51 | INFO | train_utils | Epoch 14/20 | Loss: 0.6550 | Acc: 0.7727 | Time: 25.2s\n",
      "2025-08-13 11:12:15 | INFO | train_utils | Epoch 15/20 | Loss: 0.6343 | Acc: 0.7794 | Time: 23.8s\n",
      "2025-08-13 11:12:40 | INFO | train_utils | Epoch 16/20 | Loss: 0.6175 | Acc: 0.7865 | Time: 25.6s\n",
      "2025-08-13 11:13:04 | INFO | train_utils | Epoch 17/20 | Loss: 0.6042 | Acc: 0.7904 | Time: 24.2s\n",
      "2025-08-13 11:13:31 | INFO | train_utils | Epoch 18/20 | Loss: 0.5939 | Acc: 0.7929 | Time: 26.5s\n",
      "2025-08-13 11:13:55 | INFO | train_utils | Epoch 19/20 | Loss: 0.5716 | Acc: 0.8005 | Time: 24.4s\n",
      "2025-08-13 11:14:19 | INFO | train_utils | Epoch 20/20 | Loss: 0.5588 | Acc: 0.8063 | Time: 24.2s\n",
      "2025-08-13 11:14:19 | INFO | train_utils | Training completed in 481.7s\n",
      "2025-08-13 11:14:19 | INFO | nb01 | Saved epoch history: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\metrics\\mobilenetv3_train_history.csv\n",
      "2025-08-13 11:14:19 | INFO | nb01 | [save] Wrote C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\pytorch\\mobilenetv3_cifar10.pt and C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\pytorch\\mobilenetv3_cifar10_hist.pkl\n",
      "2025-08-13 11:14:20 | INFO | nb01 | [save] Wrote C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\mobilenetv3_cifar10.onnx\n",
      "2025-08-13 11:14:49 | INFO | nb01 | [train] Training efficientnetlite0 on cuda with input shape [3, 32, 32]\n",
      "2025-08-13 11:15:24 | INFO | train_utils | Epoch 1/20 | Loss: 1.6943 | Acc: 0.3738 | Time: 35.5s\n",
      "2025-08-13 11:16:00 | INFO | train_utils | Epoch 2/20 | Loss: 1.2651 | Acc: 0.5462 | Time: 35.2s\n",
      "2025-08-13 11:16:35 | INFO | train_utils | Epoch 3/20 | Loss: 1.0380 | Acc: 0.6360 | Time: 35.3s\n",
      "2025-08-13 11:17:10 | INFO | train_utils | Epoch 4/20 | Loss: 0.8810 | Acc: 0.6943 | Time: 35.1s\n",
      "2025-08-13 11:17:44 | INFO | train_utils | Epoch 5/20 | Loss: 0.8416 | Acc: 0.7078 | Time: 34.3s\n",
      "2025-08-13 11:18:18 | INFO | train_utils | Epoch 6/20 | Loss: 0.7139 | Acc: 0.7544 | Time: 33.8s\n",
      "2025-08-13 11:18:53 | INFO | train_utils | Epoch 7/20 | Loss: 0.6661 | Acc: 0.7719 | Time: 34.8s\n",
      "2025-08-13 11:19:28 | INFO | train_utils | Epoch 8/20 | Loss: 0.6128 | Acc: 0.7900 | Time: 35.2s\n",
      "2025-08-13 11:20:03 | INFO | train_utils | Epoch 9/20 | Loss: 0.5956 | Acc: 0.7978 | Time: 35.1s\n",
      "2025-08-13 11:20:39 | INFO | train_utils | Epoch 10/20 | Loss: 0.5585 | Acc: 0.8080 | Time: 35.6s\n",
      "2025-08-13 11:21:13 | INFO | train_utils | Epoch 11/20 | Loss: 0.5226 | Acc: 0.8206 | Time: 34.1s\n",
      "2025-08-13 11:21:50 | INFO | train_utils | Epoch 12/20 | Loss: 0.5324 | Acc: 0.8186 | Time: 36.9s\n",
      "2025-08-13 11:22:24 | INFO | train_utils | Epoch 13/20 | Loss: 0.4710 | Acc: 0.8404 | Time: 34.3s\n",
      "2025-08-13 11:22:59 | INFO | train_utils | Epoch 14/20 | Loss: 0.4454 | Acc: 0.8473 | Time: 35.1s\n",
      "2025-08-13 11:23:37 | INFO | train_utils | Epoch 15/20 | Loss: 0.4395 | Acc: 0.8489 | Time: 38.0s\n",
      "2025-08-13 11:24:12 | INFO | train_utils | Epoch 16/20 | Loss: 0.4656 | Acc: 0.8414 | Time: 34.4s\n",
      "2025-08-13 11:24:47 | INFO | train_utils | Epoch 17/20 | Loss: 0.4144 | Acc: 0.8558 | Time: 35.7s\n",
      "2025-08-13 11:25:22 | INFO | train_utils | Epoch 18/20 | Loss: 0.4201 | Acc: 0.8551 | Time: 34.7s\n",
      "2025-08-13 11:25:57 | INFO | train_utils | Epoch 19/20 | Loss: 0.3810 | Acc: 0.8693 | Time: 35.3s\n",
      "2025-08-13 11:26:32 | INFO | train_utils | Epoch 20/20 | Loss: 0.3884 | Acc: 0.8653 | Time: 34.8s\n",
      "2025-08-13 11:26:32 | INFO | train_utils | Training completed in 703.3s\n",
      "2025-08-13 11:26:32 | INFO | nb01 | Saved epoch history: C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\metrics\\efficientnetlite0_train_history.csv\n",
      "2025-08-13 11:26:32 | INFO | nb01 | [save] Wrote C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\pytorch\\efficientnetlite0_cifar10.pt and C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\pytorch\\efficientnetlite0_cifar10_hist.pkl\n",
      "2025-08-13 11:26:33 | INFO | nb01 | [save] Wrote C:\\Users\\padul\\OneDrive\\Universidad\\Doctorado\\Desarrollo\\federated-lab-multihw\\models_saved\\onnx\\efficientnetlite0_cifar10.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Training / Export / Baseline\n",
    "import os, pickle\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "from torch import optim\n",
    "from utils.data_utils import DataLoaderFactory\n",
    "from utils.train_utils import train_model\n",
    "from utils.io import csv_append_row, CSV_SCHEMA, sha256_file, utc_timestamp, git_commit_short, nvidia_driver_version\n",
    "from utils.infer_torch import benchmark_dataloader\n",
    "from utils.logging_utils import get_logger\n",
    "from models.cnn import CNN\n",
    "from models.mlp import MLP\n",
    "from models.mobilenetv3 import MobileNetV3\n",
    "from models.efficientnet_lite0 import EfficientNetLite0\n",
    "\n",
    "logger = get_logger(\"nb01\")\n",
    "\n",
    "# Force flags (can also be set via environment variables)\n",
    "FORCE_RETRAIN = bool(int(os.getenv('FORCE_RETRAIN', '0')))\n",
    "FORCE_REEXPORT_ONNX = bool(int(os.getenv('FORCE_REEXPORT_ONNX', '0')))\n",
    "\n",
    "root = Path(__file__).resolve().parent.parent if '__file__' in globals() else Path(os.getcwd()).parent\n",
    "cfg = yaml.safe_load(open(root / 'config/bench_matrix.yaml', 'r', encoding='utf-8'))\n",
    "defs = cfg['defaults']\n",
    "train_cfg = cfg['train']\n",
    "models_cfg = {m['name']: m for m in cfg['models']}\n",
    "out_train_csv = root / cfg['outputs']['train_csv']\n",
    "models_pt_dir = root / 'models_saved/pytorch'\n",
    "models_onnx_dir = root / 'models_saved/onnx'\n",
    "models_pt_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_onnx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Devices and seed\n",
    "device_cuda = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_cpu = torch.device('cpu')\n",
    "seed = defs.get('seed', 42)\n",
    "torch.manual_seed(seed)\n",
    "logger.info(\"Config loaded. batch=%s, epochs=%s, seed=%s\", defs['batch'], train_cfg['epochs'], seed)\n",
    "\n",
    "# Data: CIFAR-10 loaders\n",
    "try:\n",
    "    train_loader, test_loader = DataLoaderFactory.get_cifar10_dataloaders(\n",
    "        batch_size=defs['batch'], num_workers=defs['num_workers'], data_dir=str(root / 'data'), download=True\n",
    "    )\n",
    "except Exception as ex:\n",
    "    logger.exception(\"Failed to prepare data loaders: %s\", ex)\n",
    "    raise\n",
    "\n",
    "\n",
    "def build_model(name: str):\n",
    "    if name == 'cnn': return CNN()\n",
    "    if name == 'mlp': return MLP(input_size=32 * 32 * 3)\n",
    "    if name == 'mobilenetv3': return MobileNetV3()\n",
    "    if name == 'efficientnetlite0': return EfficientNetLite0()\n",
    "    raise ValueError(f'Unknown model {name}')\n",
    "\n",
    "\n",
    "def train_and_save(name: str):\n",
    "    pt_path = models_pt_dir / models_cfg[name]['file_pt']\n",
    "    hist_path = models_pt_dir / models_cfg[name]['file_hist']\n",
    "    if pt_path.exists() and hist_path.exists() and not FORCE_RETRAIN:\n",
    "        logger.info(\"[cache] Skipping training for %s: found %s and %s\", name, pt_path.name, hist_path.name)\n",
    "        m = build_model(name)\n",
    "        state = torch.load(pt_path, map_location='cpu')\n",
    "        m.load_state_dict(state['model_state_dict'])\n",
    "        return m\n",
    "\n",
    "    m = build_model(name)\n",
    "    device = device_cuda if torch.cuda.is_available() else device_cpu\n",
    "    m = m.to(device)\n",
    "    logger.info(\"[train] Training %s on %s with input shape %s\", name, device, models_cfg[name]['input_shape'])\n",
    "    opt = optim.Adam(m.parameters(), lr=train_cfg['lr'], weight_decay=train_cfg['weight_decay'])\n",
    "    hist = train_model(\n",
    "        m, opt, train_loader, device,\n",
    "        num_epochs=train_cfg['epochs'],\n",
    "        early_stopping_patience=train_cfg['early_stopping_patience'],\n",
    "        verbose=True,\n",
    "        measure_energy=True,\n",
    "    )\n",
    "    # Save epoch-wise history to CSV for plots\n",
    "    hist_csv = root / 'metrics' / f'{name}_train_history.csv'\n",
    "    from utils.io import ensure_dir\n",
    "    ensure_dir(str(hist_csv.parent))\n",
    "    import csv\n",
    "    with open(hist_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        header = ['epoch', 'loss', 'accuracy', 'epoch_time_s']\n",
    "        if 'epoch_energy_j' in hist: header.append('epoch_energy_j')\n",
    "        w.writerow(header)\n",
    "        for i in range(len(hist['loss'])):\n",
    "            row = [i + 1, hist['loss'][i], hist['accuracy'][i], hist['epoch_time'][i]]\n",
    "            if 'epoch_energy_j' in hist: row.append(hist['epoch_energy_j'][i])\n",
    "            w.writerow(row)\n",
    "    logger.info(\"Saved epoch history: %s\", hist_csv)\n",
    "    # Save weights (.pt) and history (.pkl)\n",
    "    torch.save({'model_state_dict': m.state_dict()}, pt_path)\n",
    "    with open(hist_path, 'wb') as f:\n",
    "        pickle.dump(hist, f)\n",
    "    logger.info(\"[save] Wrote %s and %s\", pt_path, hist_path)\n",
    "    return m.cpu()\n",
    "\n",
    "\n",
    "def export_onnx(name: str, model):\n",
    "    onnx_path = models_onnx_dir / models_cfg[name]['file_onnx']\n",
    "    if onnx_path.exists() and not FORCE_REEXPORT_ONNX:\n",
    "        logger.info(\"[cache] Skipping ONNX export for %s: found %s\", name, onnx_path.name)\n",
    "        return onnx_path\n",
    "\n",
    "    chw = models_cfg[name]['input_shape']\n",
    "    sample_shape = (1, chw[0], chw[1], chw[2])\n",
    "    try:\n",
    "        if hasattr(model, 'to_onnx'):\n",
    "            model.to_onnx(sample_shape, str(onnx_path), opset=17, dynamic_batch=True)\n",
    "        else:\n",
    "            torch.onnx.export(\n",
    "                model.eval(), torch.randn(*sample_shape), str(onnx_path),\n",
    "                input_names=[\"input\"], output_names=[\"logits\"],\n",
    "                dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
    "                opset_version=17, do_constant_folding=True\n",
    "            )\n",
    "        logger.info(\"[save] Wrote %s\", onnx_path)\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"ONNX export failed for %s: %s\", name, ex)\n",
    "        raise\n",
    "    return onnx_path\n",
    "\n",
    "\n",
    "def log_baseline(name: str, onnx_path: Path):\n",
    "    rows = []\n",
    "    for dev in ['cpu', 'cuda']:\n",
    "        if dev == 'cuda' and not torch.cuda.is_available():\n",
    "            continue\n",
    "        model = build_model(name)\n",
    "        state = torch.load(models_pt_dir / models_cfg[name]['file_pt'], map_location=dev)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        device = torch.device(dev)\n",
    "        metrics = benchmark_dataloader(\n",
    "            model, test_loader, device=device,\n",
    "            warmup=defs['warmup'], runs=min(defs['runs'], len(test_loader))\n",
    "        )\n",
    "        row = {\n",
    "            'ts': utc_timestamp(), 'exp_id': 'train-baseline', 'model': name, 'dataset': defs['dataset'],\n",
    "            'precision': defs['precision'], 'engine': 'pytorch', 'provider': dev.upper(), 'batch': defs['batch'],\n",
    "            'warmup': defs['warmup'], 'runs': defs['runs'], 'lat_ms_mean': metrics['lat_ms_mean'],\n",
    "            'lat_ms_p95': metrics['lat_ms_p95'], 'thr_ips': metrics['thr_ips'], 'acc': metrics['acc'],\n",
    "            'energy_j': '', 'device_name': torch.cuda.get_device_name(0) if dev == 'cuda' else 'CPU',\n",
    "            'driver_ver': nvidia_driver_version() if dev == 'cuda' else 'N/A', 'commit': git_commit_short(),\n",
    "            'model_hash': sha256_file(str(onnx_path))\n",
    "        }\n",
    "        rows.append(row)\n",
    "    for r in rows:\n",
    "        csv_append_row(str(out_train_csv), r, CSV_SCHEMA)\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for name in models_cfg.keys():\n",
    "    model = train_and_save(name)\n",
    "    onnx_path = export_onnx(name, model)\n",
    "    log_baseline(name, onnx_path)\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
